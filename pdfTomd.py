import asyncio
import os
import re
import aiofiles
from pathlib import Path
from typing import MutableMapping
from docling.document_converter import DocumentConverter
from docling_core.transforms.chunker import HybridChunker
from docling_core.transforms.chunker.tokenizer.huggingface import HuggingFaceTokenizer
# å¯¼å…¥å¿…è¦çš„é…ç½®ç±»
from docling.datamodel.pipeline_options import PdfPipelineOptions
from docling.document_converter import DocumentConverter, PdfFormatOption

import lancedb
from lancedb.pydantic import LanceModel, Vector
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer
import pyarrow as pa

# âœ… æ ¸å¿ƒï¼šè¿‡æ»¤ transformers çš„è­¦å‘Šï¼ˆå¢å¼ºç‰ˆï¼‰
import warnings
import logging

# è¿‡æ»¤æ‰€æœ‰ä¸ token length ç›¸å…³çš„è­¦å‘Š
warnings.filterwarnings('ignore', message='Token indices sequence length')
warnings.filterwarnings('ignore', message='.*Token indices.*')
warnings.filterwarnings('ignore', category=UserWarning, module='transformers')

# åŒæ—¶é™ä½ transformers çš„æ—¥å¿—çº§åˆ«
logging.getLogger("transformers").setLevel(logging.ERROR)


# BGE-small åµŒå…¥æ¨¡å‹ç±»ï¼ˆè½»é‡é«˜æ•ˆï¼Œ512 tokens å®Œå…¨å¤Ÿç”¨ï¼‰
class BGEEmbeddingFunction:
    def __init__(self, model_name: str = "BAAI/bge-small-zh-v1.5"):
        self.model = SentenceTransformer(model_name, trust_remote_code=True)
        # âœ… è®¾ç½®æœ€å¤§åºåˆ—é•¿åº¦ï¼Œè¶…è¿‡ä¼šè‡ªåŠ¨æˆªæ–­
        self.model.max_seq_length = 512
        
    def ndims(self):
        return self.model.get_sentence_embedding_dimension()
    
    def compute_source_embeddings(self, texts):
        """è®¡ç®—æ–‡æœ¬åµŒå…¥ - è¶…è¿‡ 512 tokens ä¼šè‡ªåŠ¨æˆªæ–­"""
        return self.model.encode(texts, convert_to_numpy=True)
    
    def compute_query_embeddings(self, query):
        """è®¡ç®—æŸ¥è¯¢åµŒå…¥ - è¶…è¿‡ 512 tokens ä¼šè‡ªåŠ¨æˆªæ–­"""
        return self.model.encode([query], convert_to_numpy=True)[0]

# 1. åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ï¼ˆæ”¹ç”¨ BGE-smallï¼‰
EMBED_MODEL_ID = "BAAI/bge-small-zh-v1.5"
embedding_func = BGEEmbeddingFunction(EMBED_MODEL_ID)

class FinancialReports(LanceModel):
    vector: Vector(embedding_func.ndims())
    text: str
    source: str
    filename: str
    stock_code: str      # è‚¡ç¥¨ä»£ç ï¼Œå¦‚ï¼š002927
    annual: str          # å¹´ä»½ï¼Œå¦‚ï¼š2024
    report_type: str     # æŠ¥å‘Šç±»å‹ï¼šä¸€å­£åº¦æŠ¥ã€åŠå¹´æŠ¥ã€ä¸‰å­£åº¦æŠ¥ã€å¹´æŠ¥
    chunk_index: int     # æ–‡æœ¬å—ç´¢å¼•
    pages: str           # é¡µç åˆ—è¡¨
    section: str         # ç« èŠ‚è·¯å¾„

async def process_pdf_to_vector_task(task_id: str, pdf_path: str, tasks_registry: MutableMapping):

    """
    æ ¸å¿ƒè½¬æ¢é€»è¾‘ï¼šPDF -> Markdown -> LanceDB

    """
    try:
        # 1. ç²¾ç»†åŒ–é…ç½®å‚æ•°
        pipeline_options = PdfPipelineOptions()
        pipeline_options.do_ocr = False  # ç¦ç”¨ OCR
        pipeline_options.do_formula_enrichment = False  # ç¦ç”¨å…¬å¼è¯†åˆ«
        
        # é’ˆå¯¹è´¢æŠ¥ï¼Œä¿ç•™è¡¨æ ¼ç»“æ„è¯†åˆ«
        pipeline_options.do_table_structure = True 

        # 2. åˆå§‹åŒ–è½¬æ¢å™¨ï¼Œæ³¨å…¥é…ç½®
        converter = DocumentConverter(
            format_options={
                "pdf": PdfFormatOption(pipeline_options=pipeline_options)
            }
        )

        # 1. è½¬æ¢ PDF (è€—æ—¶æ“ä½œï¼Œæ”¾å…¥çº¿ç¨‹æ± )
        # converter = DocumentConverter()
        
        tasks_registry[task_id]["progress"] = "Converting PDF (Docling)..."
        
        # ä½¿ç”¨ to_thread é˜²æ­¢é˜»å¡äº‹ä»¶å¾ªç¯
        result = await asyncio.to_thread(converter.convert, pdf_path)

        # 2. å¯¼å‡ºå¹¶ä¿å­˜ Markdown
        tasks_registry[task_id]["progress"] = "Saving Markdown..."
        markdown_content = result.document.export_to_markdown()

        pdf_p = Path(pdf_path)

        md_path = pdf_p.parent.parent.joinpath("md")
        md_path.mkdir(parents=True, exist_ok=True)
        md_name = pdf_p.name.replace(".pdf", ".md")
        md_abs_path = md_path.joinpath(md_name)

        async with aiofiles.open(md_abs_path, "w", encoding="utf-8") as f:
            await f.write(markdown_content)

        # 3. å‘é‡åŒ–å…¥åº“ (è€—æ—¶æ“ä½œ)
        tasks_registry[task_id]["progress"] = "Embedding and Indexing..."

        def run_indexing():
            # âœ… ç»Ÿä¸€ä¿å­˜åˆ° output/lancedb ç›®å½•
            lancedb_dir = Path(__file__).parent.joinpath("output", "lancedb")
            lancedb_dir.mkdir(parents=True, exist_ok=True)
            db = lancedb.connect(str(lancedb_dir))
            
            # âœ… è¡¨åæ˜ç¡®æ ‡è¯†ä½¿ç”¨ BGE æ¨¡å‹
            table_name = "financial_reports_bge"

            # âœ… ä»æ–‡ä»¶åä¸­æå–å…ƒæ•°æ®ï¼ˆè‚¡ç¥¨ä»£ç _å¹´ä»½_æŠ¥å‘Šç±»å‹ï¼‰
            # ç¤ºä¾‹æ–‡ä»¶åï¼š002927_æ³°æ°¸é•¿å¾_2024å¹´ä¸‰å­£åº¦æŠ¥å‘Š.pdf
            name_parts = re.findall(r'\d+', md_name)
            s_code = name_parts[0] if len(name_parts) > 0 else "unknown"
            s_year = name_parts[1] if len(name_parts) > 1 else "unknown"
            
            # âœ… ç»Ÿä¸€æŠ¥å‘Šç±»å‹å‘½åï¼šä¸€å­£åº¦æŠ¥ã€åŠå¹´æŠ¥ã€ä¸‰å­£åº¦æŠ¥ã€å¹´æŠ¥
            if "åŠå¹´åº¦" in md_name or "åŠå¹´åº¦æŠ¥" in md_name:
                r_type = "åŠå¹´åº¦æŠ¥"
            elif "å¹´åº¦æŠ¥å‘Š" in md_name or "å¹´æŠ¥" in md_name:
                r_type = "å¹´æŠ¥"
            elif "ä¸€å­£åº¦" in md_name or "ç¬¬ä¸€å­£åº¦" in md_name:
                r_type = "ä¸€å­£åº¦æŠ¥"
            elif "ä¸‰å­£åº¦" in md_name or "ç¬¬ä¸‰å­£åº¦" in md_name:
                r_type = "ä¸‰å­£åº¦æŠ¥"
            else:
                r_type = "unknown"
            
            print(f"ğŸ“Š å‘é‡åŒ–å…¥åº“ï¼š{s_code}_{s_year}_{r_type}")

            # âœ… ä½¿ç”¨ BGE-smallï¼Œmax_tokens=512 å®Œå…¨å¤Ÿç”¨
            # æ˜¾å¼æŒ‡å®š tokenizerï¼Œé¿å…é»˜è®¤ tokenizer çš„è¯¯æŠ¥è­¦å‘Š
            tokenizer = HuggingFaceTokenizer(
                tokenizer=AutoTokenizer.from_pretrained(EMBED_MODEL_ID),
                max_tokens=512,
            )
            chunker = HybridChunker(
                tokenizer=tokenizer,
                merge_peers=True,
            )

            chunks = list(chunker.chunk(result.document))

            # âœ… è¾…åŠ©å‡½æ•°ï¼šæ™ºèƒ½æˆªæ–­é•¿æ–‡æœ¬ï¼ˆå¯é€‰ï¼Œå¦‚æœè­¦å‘Šä»ç„¶å‡ºç°ï¼‰
            def smart_truncate(text: str, max_tokens: int = 512) -> str:
                """
                æ™ºèƒ½æˆªæ–­æ–‡æœ¬åˆ°æŒ‡å®š token æ•°
                å¦‚æœæ–‡æœ¬ä¸è¶…é•¿ï¼Œç›´æ¥è¿”å›ï¼›å¦åˆ™æˆªæ–­åˆ° max_tokens
                """
                tokenizer_local = AutoTokenizer.from_pretrained(EMBED_MODEL_ID)
                tokens = tokenizer_local.encode(text, add_special_tokens=False)
                
                if len(tokens) <= max_tokens:
                    return text  # ä¸éœ€è¦æˆªæ–­
                
                # æˆªæ–­å¹¶è§£ç 
                truncated_tokens = tokens[:max_tokens]
                return tokenizer_local.decode(truncated_tokens, skip_special_tokens=True)

            data = []
            for i, chunk in enumerate(chunks):
                chunk_text = chunker.contextualize(chunk)
                
                # âœ… å¯é€‰ï¼šå¦‚æœè¿˜æ˜¯æœ‰è­¦å‘Šï¼Œå–æ¶ˆä¸‹é¢çš„æ³¨é‡Š
                # chunk_text = smart_truncate(chunk_text, max_tokens=480)  # ç•™ä¸€ç‚¹ä½™é‡
                
                page_numbers = list(set(p.page_no for item in chunk.meta.doc_items for p in item.prov))

                # æ ¸å¿ƒä¼˜åŒ–ï¼šå¢åŠ ç²¾å‡†è¿‡æ»¤å­—æ®µ
                data.append({
                    "text": chunk_text,
                    "vector": embedding_func.compute_source_embeddings([chunk_text])[0],  # æ‰‹åŠ¨è®¡ç®—åµŒå…¥
                    "source": str(md_abs_path),
                    "filename": md_name,
                    "stock_code": s_code,
                    "annual": s_year,
                    "report_type": r_type,
                    "chunk_index": i,
                    "pages": str(page_numbers),
                    "section": " > ".join(chunk.meta.headings) if chunk.meta.headings else "æ­£æ–‡"
                })

            # Note: list_tables() è¿”å› ListTablesResponse å¯¹è±¡ï¼Œéœ€è¦è®¿é—® .tables å±æ€§
            table_names = db.list_tables().tables if hasattr(db.list_tables(), 'tables') else []
            
            if table_name in table_names:
                table = db.open_table(table_name)
                
                # âœ… æ™ºèƒ½å»é‡ï¼šæ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨è¯¥æŠ¥å‘Šæ•°æ®
                try:
                    existing = table.search() \
                        .where(f"stock_code = '{s_code}'") \
                        .where(f"annual = '{s_year}'") \
                        .where(f"report_type = '{r_type}'") \
                        .limit(1) \
                        .to_list()
                    
                    if existing:
                        # å­˜åœ¨æ—§æ•°æ®ï¼Œå…ˆåˆ é™¤
                        print(f"âš ï¸ å‘ç°å·²å­˜åœ¨æ•°æ®ï¼š{s_code}_{s_year}_{r_type}ï¼Œåˆ é™¤æ—§æ•°æ®...")
                        table.delete(f"stock_code = '{s_code}' AND annual = '{s_year}' AND report_type = '{r_type}'")
                        print(f"âœ… è¿½åŠ æ–°æ•°æ®ï¼š{s_code}_{s_year}_{r_type} ({len(data)} chunks)")
                        table.add(data)
                    else:
                        # ä¸å­˜åœ¨ï¼Œç›´æ¥è¿½åŠ 
                        print(f"âœ… è¿½åŠ æ–°æ•°æ®ï¼š{s_code}_{s_year}_{r_type} ({len(data)} chunks)")
                        table.add(data)
                except Exception as e:
                    # æŸ¥è¯¢å¤±è´¥ï¼ˆå¯èƒ½æ˜¯è¡¨ç»“æ„å˜åŒ–ï¼‰ï¼Œç›´æ¥è¿½åŠ 
                    print(f"âš ï¸ æŸ¥è¯¢å¤±è´¥ï¼Œç›´æ¥è¿½åŠ æ•°æ®ï¼š{e}")
                    table.add(data)
            else:
                # é¦–æ¬¡åˆ›å»ºè¡¨
                print(f"âœ… åˆ›å»ºæ–°è¡¨ï¼š{table_name}")
                db.create_table(table_name, schema=FinancialReports, data=data)
            
            return len(data)


        # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œé‡åº¦è®¡ç®—
        doc_count = await asyncio.to_thread(run_indexing)

        # 4. æˆåŠŸå›è°ƒ
        tasks_registry[task_id].update({
            "status": "completed",
            "progress": "100%",
            "result": f"Success: {md_name} processed. {doc_count} chunks indexed.",
            "md_path": str(md_abs_path)
        })

    except Exception as e:
        tasks_registry[task_id].update({
            "status": "failed",
            "progress": "Error",
            "result": f"Failed: {str(e)}"
        })