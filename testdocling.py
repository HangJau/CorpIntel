


import time, asyncio, os, re
from pathlib import Path
import aiofiles
from docling.document_converter import DocumentConverter
from docling_core.transforms.chunker import HybridChunker
from docling_core.transforms.chunker.tokenizer.huggingface import HuggingFaceTokenizer
# å¯¼å…¥å¿…è¦çš„é…ç½®ç±»
from docling.datamodel.pipeline_options import PdfPipelineOptions
from docling.document_converter import DocumentConverter, PdfFormatOption

import lancedb
from lancedb.pydantic import LanceModel, Vector
from lancedb.embeddings import EmbeddingFunctionRegistry
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer
import pyarrow as pa

# âœ… æ ¸å¿ƒï¼šè¿‡æ»¤ transformers çš„è­¦å‘Šï¼ˆå¢å¼ºç‰ˆï¼‰
import warnings
import logging

# è¿‡æ»¤æ‰€æœ‰ä¸ token length ç›¸å…³çš„è­¦å‘Š
warnings.filterwarnings('ignore', message='Token indices sequence length')
warnings.filterwarnings('ignore', message='.*Token indices.*')
warnings.filterwarnings('ignore', category=UserWarning, module='transformers')

# åŒæ—¶é™ä½ transformers çš„æ—¥å¿—çº§åˆ«
logging.getLogger("transformers").setLevel(logging.ERROR)


# æ›¿æ¢ä¸ºä½ æœ¬åœ°çš„ä¸€ä»½è´¢æŠ¥è·¯å¾„
source = r"D:\Temp\mycode\CorpIntel\output\pdf\(002927)æ³°æ°¸é•¿å¾ï¼š2025å¹´ä¸‰å­£åº¦æŠ¥å‘Š.pdf"

save_path = r'D:\Temp\mycode\CorpIntel\output\md\(002927)æ³°æ°¸é•¿å¾ï¼š2025å¹´ä¸‰å­£åº¦æŠ¥å‘Š.md'

async def test():

    # 1. ç²¾ç»†åŒ–é…ç½®å‚æ•°
    pipeline_options = PdfPipelineOptions()
    pipeline_options.do_ocr = False  # ç¦ç”¨ OCR
    pipeline_options.do_formula_enrichment = False  # ç¦ç”¨å…¬å¼è¯†åˆ«
    
    # é’ˆå¯¹è´¢æŠ¥ï¼Œä¿ç•™è¡¨æ ¼ç»“æ„è¯†åˆ«
    pipeline_options.do_table_structure = True 

    # 2. åˆå§‹åŒ–è½¬æ¢å™¨ï¼Œæ³¨å…¥é…ç½®
    converter = DocumentConverter(
        format_options={
            "pdf": PdfFormatOption(pipeline_options=pipeline_options)
        }
    )
    
    result = converter.convert(source)

    print("PDF TO MD å¼€å§‹æ—¶é—´ï¼š", time.time())

    markdown_content = result.document.export_to_markdown()
    print("PDF TO MD ç»“æŸæ—¶é—´ï¼š", time.time())
    # async with aiofiles.open(save_path, "w", encoding="utf-8") as f:
    #         await f.write(markdown_content)
    #         await f.close()
    
    # 3. å‘é‡åŒ–å…¥åº“ (è€—æ—¶æ“ä½œ)
    print("å‘é‡åŒ–å…¥åº“æ—¶é—´ï¼š", time.time())

    md_name = Path(save_path).name

    # âœ… ç»Ÿä¸€ä¿å­˜åˆ° output/lancedb ç›®å½•
    lancedb_dir = Path(__file__).parent.joinpath("output")
    lancedb_dir.mkdir(parents=True, exist_ok=True)
    db = lancedb.connect(str(lancedb_dir))
    table_name = "financial_reports_bge"

    # BGE-small åµŒå…¥æ¨¡å‹ç±»ï¼ˆè½»é‡é«˜æ•ˆï¼Œ512 tokens å®Œå…¨å¤Ÿç”¨ï¼‰
    class BGEEmbeddingFunction:
        def __init__(self, model_name: str = "BAAI/bge-small-zh-v1.5"):
            print(f"{'='*60}")
            print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_name}")
            print(f"ã€æ¨¡å‹è¯´æ˜ã€‘BGE-small è½»é‡é«˜æ•ˆï¼Œmax_seq_length=512")
            print(f"{'='*60}")
            
            self.model = SentenceTransformer(model_name, trust_remote_code=True)
            
            # âœ… æ ¸å¿ƒé…ç½®ï¼šè®¾ç½®æœ€å¤§åºåˆ—é•¿åº¦ä¸º 512
            # è¿™æ · encode æ—¶ä¼šè‡ªåŠ¨æˆªæ–­ï¼Œä¸ä¼šæŠ¥è­¦å‘Š
            self.model.max_seq_length = 512
            
            print(f"ã€æ¨¡å‹é…ç½®ã€‘")
            print(f"  model.max_seq_length: {self.model.max_seq_length}")
            if hasattr(self.model, 'tokenizer') and self.model.tokenizer is not None:
                print(f"  tokenizer.model_max_length: {self.model.tokenizer.model_max_length}")
            print(f"{'='*60}")
            
        def ndims(self):
            return self.model.get_sentence_embedding_dimension()
        
        def compute_source_embeddings(self, texts):
            """è®¡ç®—æ–‡æœ¬åµŒå…¥ - è¶…è¿‡ 512 tokens ä¼šè‡ªåŠ¨æˆªæ–­"""
            # æ³¨æ„ï¼šmax_seq_length=512 å·²åœ¨ __init__ ä¸­è®¾ç½®ï¼Œè¿™é‡Œä¼šè‡ªåŠ¨æˆªæ–­
            return self.model.encode(
                texts, 
                convert_to_numpy=True,
                show_progress_bar=False,
                batch_size=1
            )
        
        def compute_query_embeddings(self, query):
            """è®¡ç®—æŸ¥è¯¢åµŒå…¥ - è¶…è¿‡ 512 tokens ä¼šè‡ªåŠ¨æˆªæ–­"""
            return self.model.encode(
                [query], 
                convert_to_numpy=True,
                show_progress_bar=False
            )[0]

    # åˆå§‹åŒ–åµŒå…¥å‡½æ•°ï¼ˆæ”¹ç”¨ BGE-smallï¼‰
    EMBED_MODEL_ID = "BAAI/bge-small-zh-v1.5"
    embedding_func = BGEEmbeddingFunction(EMBED_MODEL_ID)

    class FinancialReportsBGE(LanceModel):
        vector: Vector(embedding_func.ndims())
        text: str
        source: str
        filename: str
        stock_code: str      # è‚¡ç¥¨ä»£ç ï¼Œå¦‚ï¼š002927
        annual: str          # å¹´ä»½ï¼Œå¦‚ï¼š2024
        report_type: str     # æŠ¥å‘Šç±»å‹ï¼šä¸€å­£åº¦æŠ¥ã€åŠå¹´æŠ¥ã€ä¸‰å­£åº¦æŠ¥ã€å¹´æŠ¥
        chunk_index: int     # æ–‡æœ¬å—ç´¢å¼•
        pages: str           # é¡µç åˆ—è¡¨
        section: str         # ç« èŠ‚è·¯å¾„

    # âœ… ä»æ–‡ä»¶åä¸­æå–å…ƒæ•°æ®ï¼ˆè‚¡ç¥¨ä»£ç _å¹´ä»½_æŠ¥å‘Šç±»å‹ï¼‰
    # ç¤ºä¾‹æ–‡ä»¶åï¼š002927_æ³°æ°¸é•¿å¾_2025å¹´ä¸‰å­£åº¦æŠ¥å‘Š.pdf
    name_parts = re.findall(r'\d+', md_name)

    s_code = name_parts[0] if len(name_parts) > 0 else "unknown"
    s_year = name_parts[1] if len(name_parts) > 1 else "unknown"
    
    # âœ… ç»Ÿä¸€æŠ¥å‘Šç±»å‹å‘½åï¼šä¸€å­£åº¦æŠ¥ã€åŠå¹´æŠ¥ã€ä¸‰å­£åº¦æŠ¥ã€å¹´æŠ¥
    if "åŠå¹´åº¦" in md_name or "åŠå¹´æŠ¥" in md_name:
        r_type = "åŠå¹´åº¦æŠ¥"
    elif "å¹´åº¦æŠ¥å‘Š" in md_name or "å¹´æŠ¥" in md_name:
        r_type = "å¹´æŠ¥"
    elif "ä¸€å­£åº¦" in md_name or "ç¬¬ä¸€å­£åº¦" in md_name:
        r_type = "ä¸€å­£åº¦æŠ¥"
    elif "ä¸‰å­£åº¦" in md_name or "ç¬¬ä¸‰å­£åº¦" in md_name:
        r_type = "ä¸‰å­£åº¦æŠ¥"
    else:
        r_type = "unknown"

    print(f"ğŸ“Š å‘é‡åŒ–å…¥åº“ï¼š{s_code}_{s_year}_{r_type}")


    # âœ… ä½¿ç”¨ BGE-smallï¼Œmax_tokens=512 å®Œå…¨å¤Ÿç”¨
    # æ˜¾å¼æŒ‡å®š tokenizerï¼Œé¿å…é»˜è®¤ tokenizer çš„è¯¯æŠ¥è­¦å‘Š
    print("åˆå§‹åŒ– HybridChunker...")
    tokenizer = HuggingFaceTokenizer(
        tokenizer=AutoTokenizer.from_pretrained(EMBED_MODEL_ID),
        max_tokens=512,
    )
    chunker = HybridChunker(
        tokenizer=tokenizer,
        merge_peers=True,
    )

    chunks = list(chunker.chunk(result.document))

    # âœ… è¾…åŠ©å‡½æ•°ï¼šæ™ºèƒ½æˆªæ–­é•¿æ–‡æœ¬ï¼ˆå¯é€‰ï¼Œå¦‚æœè­¦å‘Šä»ç„¶å‡ºç°ï¼‰
    def smart_truncate(text: str, max_tokens: int = 512) -> str:
        """
        æ™ºèƒ½æˆªæ–­æ–‡æœ¬åˆ°æŒ‡å®š token æ•°
        å¦‚æœæ–‡æœ¬ä¸è¶…é•¿ï¼Œç›´æ¥è¿”å›ï¼›å¦åˆ™æˆªæ–­åˆ° max_tokens
        """
        tokenizer_local = AutoTokenizer.from_pretrained(EMBED_MODEL_ID)
        tokens = tokenizer_local.encode(text, add_special_tokens=False)
        
        if len(tokens) <= max_tokens:
            return text  # ä¸éœ€è¦æˆªæ–­
        
        # æˆªæ–­å¹¶è§£ç 
        truncated_tokens = tokens[:max_tokens]
        return tokenizer_local.decode(truncated_tokens, skip_special_tokens=True)

    data = []

    for i, chunk in enumerate(chunks):
        chunk_text = chunker.contextualize(chunk)
        
        # âœ… å¯é€‰ï¼šå¦‚æœè¿˜æ˜¯æœ‰è­¦å‘Šï¼Œå–æ¶ˆä¸‹é¢çš„æ³¨é‡Š
        # chunk_text = smart_truncate(chunk_text, max_tokens=480)  # ç•™ä¸€ç‚¹ä½™é‡
        
        page_numbers = list(set(p.page_no for item in chunk.meta.doc_items for p in item.prov))

        # âœ… ç²¾ç®€å…ƒæ•°æ®ï¼šæ”¯æŒå¤šè‚¡ç¥¨ã€å¤šå¹´ä»½ã€å¤šæŠ¥å‘Šç±»å‹æŸ¥è¯¢
        data.append({
            "text": chunk_text,
            "vector": embedding_func.compute_source_embeddings([chunk_text])[0],
            "source": save_path,
            "filename": md_name,
            "stock_code": s_code,      # ç²¾å‡†è¿‡æ»¤ï¼šWHERE stock_code = '002927'
            "annual": s_year,          # ç²¾å‡†è¿‡æ»¤ï¼šWHERE annual = '2024'
            "report_type": r_type,     # ç²¾å‡†è¿‡æ»¤ï¼šWHERE report_type = 'ä¸‰å­£åº¦æŠ¥'
            "chunk_index": i,
            "pages": str(page_numbers),
            "section": " > ".join(chunk.meta.headings) if chunk.meta.headings else "æ­£æ–‡"
        })

    # âœ… æ™ºèƒ½åˆå¹¶ç­–ç•¥ï¼šæ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨åŒä¸€ä»½æŠ¥å‘Šçš„æ•°æ®
    # Note: list_tables() è¿”å› ListTablesResponse å¯¹è±¡ï¼Œéœ€è¦è®¿é—® .tables å±æ€§è·å–è¡¨ååˆ—è¡¨
    table_names = db.list_tables().tables if hasattr(db.list_tables(), 'tables') else []
    
    if table_name in table_names:
        table = db.open_table(table_name)
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨è¯¥æŠ¥å‘Šï¼ˆé€šè¿‡ stock_code + annual + report_type å”¯ä¸€æ ‡è¯†ï¼‰
        try:
            existing = table.search() \
                .where(f"stock_code = '{s_code}'") \
                .where(f"annual = '{s_year}'") \
                .where(f"report_type = '{r_type}'") \
                .limit(1) \
                .to_list()
            
            if existing:
                print(f"âš ï¸ å‘ç°å·²å­˜åœ¨æ•°æ®ï¼š{s_code}_{s_year}_{r_type}ï¼Œè¿½åŠ æ–°æ•°æ®...")
                table.add(data)
            else:
                print(f"âœ… è¿½åŠ æ–°æ•°æ®ï¼š{s_code}_{s_year}_{r_type} ({len(data)} chunks)")
                table.add(data)
        except Exception as e:
            # æŸ¥è¯¢å¤±è´¥ï¼ˆå¯èƒ½æ˜¯è¡¨ç»“æ„å˜åŒ–ï¼‰ï¼Œç›´æ¥è¿½åŠ 
            print(f"âš ï¸ æŸ¥è¯¢å¤±è´¥ï¼Œç›´æ¥è¿½åŠ æ•°æ®ï¼š{e}")
            table.add(data)
    else:
        # é¦–æ¬¡åˆ›å»ºè¡¨
        print(f"âœ… åˆ›å»ºæ–°è¡¨ï¼š{table_name}")
        db.create_table(table_name, schema=FinancialReportsBGE, data=data)

    print("å‘é‡åŒ–å…¥åº“ç»“æŸæ—¶é—´ï¼š", time.time())
    print(len(data))



if __name__ == "__main__":
    asyncio.run(test())